{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP - How Trump's Tweets impact China Market\n",
    "\n",
    "#### The Motivation of the Project\n",
    "This project is a basic investigation of the natural language process. Analyze how Trump's Tweets will impact China Market by checking what would happen in markets after Trump said positive, neutral or negative things about China. I get the idea from https://www.youtube.com/watch?v=wlnx-7cm4Gg&t=890s\n",
    "\n",
    "#### A brief summary:\n",
    "1.  Get Trump's twitter data from Twitter Developer (a token is required, so you need to sign up an account at https://developer.twitter.com/en)\n",
    "2. Clean twitter data, including:\n",
    "> - Extract main sentences by regularization\n",
    "> - Find all root words (e.g. turn 'took' into 'take')\n",
    "> - Delete stop words (e.g. deleting 'a', 'the')\n",
    "> - Filter tweets with keywords: I use China and Chinese, only the tweets Have one of the keywords will be used in further research\n",
    "> - Adjust time value: The time recode in twitter is neither US time nor China time. Turn it into China time, and if it is later than 15 p.m. (stock market close at this time), the date will lag for 1 day. (like, tonight's twitter will only influence tomorrow's market)\n",
    "> - Analyzing the sentiment of a tweet (by third-party package). If there are more than 2 related tweets, the sentiment of that day would be weighted by retweets value.\n",
    "3. Download Shanghai market data, by yahoo finance.\n",
    "4. Analyze the correlation between two data:\n",
    "> - correlation between sentiments and return of that day\n",
    "> - set classes of sentiment and return, find the correlation between two class\n",
    " \n",
    "#### Preliminary result\n",
    "1. kind of no strong correlation.\n",
    "2. An interesting thing is that I did similar steps in March (data from Dec to Mar), the correlation is strong than now.\n",
    "\n",
    "#### Problems (need to develop):\n",
    "1. The data set is small. A free account of twitter developer could fetch only 3200 tweets from a person, including replies. That's only 2 or 3 months data, cuz Trump used twitter tooooo much.\n",
    "2. The tweet on weekends was not considered. (there might be lots of news could impact markets at weekends)\n",
    "3. The analyzing part is simple and superficial.\n",
    "4. Using daily data now, minutes data would be much better.\n",
    "5. Sentiment analysis is given by a third-party package, which is not precise, sometimes.\n",
    "\n",
    "#### Possible Future Work\n",
    "1. I personally find it is an interesting NLP project, and the idea could used to other places, like how CNN's news impact market.\n",
    "2. The key point is how to combine markets and tweets, by quantitative methods. Otherwise, it is completely a data science project.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import API \n",
    "from tweepy import Cursor\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "from scipy.stats import f\n",
    "\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "# You can get key and token from https://developer.twitter.com/en\n",
    "CONSUMER_KEY    = \n",
    "CONSUMER_SECRET = \n",
    "# Access:\n",
    "ACCESS_TOKEN  = \n",
    "ACCESS_TOKEN_SECRET = \n",
    "\n",
    "\n",
    "# # # # TWITTER CLIENT # # # #\n",
    "class TwitterClient():\n",
    "    def __init__(self, twitter_user=None):\n",
    "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\n",
    "        self.twitter_client = API(self.auth)\n",
    "        self.twitter_user = twitter_user\n",
    "\n",
    "    def get_twitter_client_api(self):\n",
    "        return self.twitter_client\n",
    "\n",
    "    \n",
    "# # # # TWITTER AUTHENTICATER # # # #\n",
    "class TwitterAuthenticator():\n",
    "    def authenticate_twitter_app(self):\n",
    "        auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "        auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "        return auth\n",
    "\n",
    "    \n",
    "# # # # TWITTER STREAMER # # # #\n",
    "class TwitterStreamer():\n",
    "# monitor the live twitter, return json\n",
    "    def __init__(self):\n",
    "        self.twitter_autenticator = TwitterAuthenticator()    \n",
    "\n",
    "    def stream_tweets(self, fetched_tweets_filename, hash_tag_list):\n",
    "        # This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "        listener = TwitterListener(fetched_tweets_filename)\n",
    "        auth = self.twitter_autenticator.authenticate_twitter_app() \n",
    "        stream = Stream(auth, listener)\n",
    "        # This line filter Twitter Streams to capture data by the keywords: \n",
    "        stream.filter(track=hash_tag_list)\n",
    "\n",
    "        \n",
    "# # # # TWITTER STREAM LISTENER # # # #\n",
    "class TwitterListener(StreamListener):\n",
    "    def __init__(self, fetched_tweets_filename):\n",
    "        self.fetched_tweets_filename = fetched_tweets_filename\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            print(data)\n",
    "            with open(self.fetched_tweets_filename, 'a') as tf:\n",
    "                tf.write(data)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data %s\" % str(e))\n",
    "        return True\n",
    "          \n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            # Returning False on_data method in case rate limit occurs.\n",
    "            return False\n",
    "        print(status)\n",
    "\n",
    "\n",
    "# # # # TWITTER ANALYSIS # # # #\n",
    "class TweetAnalyzer():\n",
    "### process the historical twitter\n",
    "    def tweets_to_data_frame(self, tweets):\n",
    "        df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "        df['id'] = np.array([tweet.id for tweet in tweets])\n",
    "        df['len'] = np.array([len(tweet.text) for tweet in tweets])\n",
    "        df['date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "        df['source'] = np.array([tweet.source for tweet in tweets])\n",
    "        df['likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "        df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "        df['Tweets'] = df.Tweets.apply(self.clean_tweet)\n",
    "        return df\n",
    "    \n",
    "    # catch meaning for part, clean stop word, find root\n",
    "    def clean_tweet(self, tweet):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        words = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()\n",
    "        # kick out stop words\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        # word root\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        # words = [stemmer.stem(word) for word in words]\n",
    "        # return ' '.join(words)\n",
    "        return words\n",
    "    \n",
    "    # find key word in twitter, return 0 or 1\n",
    "    def find_key_word(self, tweet):\n",
    "        for key_word in key_word_list:\n",
    "            if key_word in tweet:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    # return sentiment, need to input a whole string\n",
    "    def analyze_sentiment(self, tweet):\n",
    "        analysis = TextBlob(\" \".join(tweet))\n",
    "        return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch live twitter\n",
    "#hash_tag_list = [\"donal trump\", \"hillary clinton\", \"barack obama\", \"bernie sanders\"]\n",
    "#fetched_tweets_filename = \"tweets.txt\"\n",
    "#twitter_streamer = TwitterStreamer()\n",
    "#twitter_streamer.stream_tweets(fetched_tweets_filename, hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download twitter data\n",
    "def fetch_twitter(name = \"realDonaldTrump\", pages = 16, key_word_list = ['China','Chinese']):\n",
    "    print('Downloading Tweets...\\t')\n",
    "    twitter_client = TwitterClient()\n",
    "    tweet_analyzer = TweetAnalyzer()\n",
    "    api = twitter_client.get_twitter_client_api()\n",
    "\n",
    "    # can only get 200*16 as maximum\n",
    "    tweets = api.user_timeline(screen_name = name, count=200, page=1)  #exclude_replies = True, include_rts=False\n",
    "    df = tweet_analyzer.tweets_to_data_frame(tweets)\n",
    "    for i in range(2, pages+1):\n",
    "        tweets = api.user_timeline(screen_name = name, count=200, page=i)\n",
    "        df = pd.concat([df,tweet_analyzer.tweets_to_data_frame(tweets)])\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # sentiment analysis\n",
    "    df['sentiment'] = df['Tweets'].apply(tweet_analyzer.analyze_sentiment)\n",
    "    # find the tweets with key words\n",
    "    df['include_keyword'] = df['Tweets'].apply(tweet_analyzer.find_key_word)\n",
    "    print('Total Tweets:',len(df),' tweets with key words:',sum(df.include_keyword))\n",
    "    # turn time into China time\n",
    "    df['effect_date'] = pd.to_datetime(df.date).apply(lambda x:(x + datetime.timedelta(hours=20)))\n",
    "    # lag for one day if the time is latter than 15 pm\n",
    "    df['effect_date'] = df.effect_date.apply(lambda x:(x+datetime.timedelta(days=1) if x.hour>15 else x).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    return df.loc[df.include_keyword==1,['Tweets','len','date','effect_date','likes','retweets','sentiment']]\n",
    "\n",
    "\n",
    "# download market data\n",
    "def fetch_stock_data(df, ret_upper = 0.003, ret_lower = -0.003):\n",
    "    print('Downloading Stock Data...\\t')\n",
    "    datelist = sorted(list(df.effect_date.unique()))\n",
    "    yf.pdr_override()\n",
    "    ss_data = pd.DataFrame(pdr.get_data_yahoo('000001.SS', start=datelist[0], end=datelist[-1])['Adj Close'])\n",
    "    ss_data['Return'] = ss_data['Adj Close']/ss_data['Adj Close'].shift(1) - 1\n",
    "    ss_data['Return_Class'] = ss_data.Return.apply(lambda x:1 if x > ret_upper else -1 if x < ret_lower else 0)\n",
    "    ss_data.index = ss_data.index.strftime(\"%Y-%m-%d\")\n",
    "    ss_data = ss_data.dropna()\n",
    "\n",
    "    return ss_data.loc[ss_data.index.isin(datelist)]\n",
    "\n",
    "\n",
    "def process(sen_upper = 0.1, sen_lower = 0.1):\n",
    "    df = fetch_twitter(name = \"realDonaldTrump\", pages = 20, key_word_list = ['China','Chinese'])\n",
    "    ret = fetch_stock_data(df, ret_upper = 0.005, ret_lower = -0.005)\n",
    "    ret['Tweets_Amount'], ret['Weighted_Sentiment'], ret['Sentiment_Class'] = 0,0,0\n",
    "    for date in ret.index:\n",
    "        tem = df.loc[df.effect_date == date]\n",
    "        ret.loc[ret.index == date,'Tweets_Amount'] = len(tem)\n",
    "        # sentiment is weighted by retweets value\n",
    "        tem_sentiment = sum(tem['retweets'] * tem['sentiment'])/sum(tem['retweets'])\n",
    "        ret.loc[ret.index == date,'Weighted_Sentiment'] = tem_sentiment\n",
    "        if tem_sentiment >= 0.05:\n",
    "            ret.loc[ret.index == date,'Sentiment_Class'] = ret.loc[ret.index == date,'Weighted_Sentiment'].apply(\\\n",
    "            lambda x:1 if x > sen_upper else -1 if x < sen_lower else 0)\n",
    "\n",
    "    return df,ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Tweets...\t\n",
      "Total Tweets: 3234  tweets with key words: 64\n",
      "Downloading Stock Data...\t\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[[ 1.         -0.13029578]\n",
      " [-0.13029578  1.        ]]\n",
      "[[1.         0.17073699]\n",
      " [0.17073699 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ is '__main__':\n",
    "    key_word_list = ['China','Chinese']\n",
    "    twitter_data, ret = process()\n",
    "    print(np.corrcoef(ret.Return,ret.Weighted_Sentiment))\n",
    "    print(np.corrcoef(ret.Return_Class,ret.Sentiment_Class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>len</th>\n",
       "      <th>date</th>\n",
       "      <th>effect_date</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>[Biden, failed, China, They, took, u, cleaner,...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>2020-06-24 22:10:48</td>\n",
       "      <td>2020-06-26</td>\n",
       "      <td>164723.0</td>\n",
       "      <td>34804.0</td>\n",
       "      <td>-0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>[We, great, job, CoronaVirus, including, early...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2020-06-23 14:41:50</td>\n",
       "      <td>2020-06-24</td>\n",
       "      <td>122105.0</td>\n",
       "      <td>25251.0</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>[The, China, Trade, Deal, fully, intact, Hopef...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2020-06-23 02:22:18</td>\n",
       "      <td>2020-06-24</td>\n",
       "      <td>121205.0</td>\n",
       "      <td>22762.0</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>[How, think, China, Russia, Japan, others, wou...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2020-06-12 12:37:47</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>75432.0</td>\n",
       "      <td>19059.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>[RT, Decades, failed, engagement, Western, inv...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2020-06-11 03:30:48</td>\n",
       "      <td>2020-06-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11639.0</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets    len  \\\n",
       "152  [Biden, failed, China, They, took, u, cleaner,...  129.0   \n",
       "175  [We, great, job, CoronaVirus, including, early...  140.0   \n",
       "189  [The, China, Trade, Deal, fully, intact, Hopef...  108.0   \n",
       "438  [How, think, China, Russia, Japan, others, wou...  140.0   \n",
       "494  [RT, Decades, failed, engagement, Western, inv...  140.0   \n",
       "\n",
       "                   date effect_date     likes  retweets  sentiment  \n",
       "152 2020-06-24 22:10:48  2020-06-26  164723.0   34804.0  -0.150000  \n",
       "175 2020-06-23 14:41:50  2020-06-24  122105.0   25251.0   0.450000  \n",
       "189 2020-06-23 02:22:18  2020-06-24  121205.0   22762.0   0.136364  \n",
       "438 2020-06-12 12:37:47  2020-06-13   75432.0   19059.0   0.000000  \n",
       "494 2020-06-11 03:30:48  2020-06-12       0.0   11639.0  -0.333333  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Return</th>\n",
       "      <th>Return_Class</th>\n",
       "      <th>Tweets_Amount</th>\n",
       "      <th>Weighted_Sentiment</th>\n",
       "      <th>Sentiment_Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>2764.910889</td>\n",
       "      <td>-0.005997</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>2815.368896</td>\n",
       "      <td>-0.001912</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>2825.904053</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-13</th>\n",
       "      <td>2783.048096</td>\n",
       "      <td>-0.004857</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-14</th>\n",
       "      <td>2827.282959</td>\n",
       "      <td>0.015894</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.286314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Adj Close    Return  Return_Class  Tweets_Amount  \\\n",
       "Date                                                             \n",
       "2020-03-26  2764.910889 -0.005997            -1              2   \n",
       "2020-04-08  2815.368896 -0.001912             0              1   \n",
       "2020-04-09  2825.904053  0.003742             0              1   \n",
       "2020-04-13  2783.048096 -0.004857             0              1   \n",
       "2020-04-14  2827.282959  0.015894             1              2   \n",
       "\n",
       "            Weighted_Sentiment  Sentiment_Class  \n",
       "Date                                             \n",
       "2020-03-26            0.015969                0  \n",
       "2020-04-08            0.138095                1  \n",
       "2020-04-09            0.000000                0  \n",
       "2020-04-13           -0.083333                0  \n",
       "2020-04-14           -0.286314                0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
